#!/usr/bin/env python3
import torch;
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
import numpy as np
import math
import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100
from cog_learning.nn_ga import *
from cog_learning.msg import LatentGoalDnf
from cog_learning.msg import Goal
import rospy
from std_msgs.msg import Float64
from std_msgs.msg import Int16
from std_msgs.msg import Bool
from motion.msg import Dmp
from motion.msg import Action
from motion.msg import DmpAction
from motion.msg import DmpOutcome
from detector.msg import Outcome
from detector.msg import State
from cluster_message.msg import SampleExplore
from cluster_message.msg import SampleExploit
from cv_bridge import CvBridge, CvBridgeError
from sensor_msgs.msg import Image
from cog_learning.srv import *
from cog_learning.msg import SamplePred, ListOutput, ListSamplePred

try:
    import cPickle as pickle
except ModuleNotFoundError:
    import pickle

is_cuda = torch.cuda.is_available()
#device = torch.device("cpu")

if not is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")

class Exploitation(object):
    def __init__(self):
        rospy.init_node('exploitation', anonymous=True)
        rospy.Service('predict_action', PredAction, self.get_prediction)
        rospy.Service('predict_inverse', PredInverse, self.get_inverse)
        rospy.Service('get_inverse_error', GetInvError, self.get_inverse_error)
        self.bridge = CvBridge()
        self.pub_field = rospy.Publisher("/cog_learning/cedar/mt", Image, queue_size=1, latch=True)
        self.pub_mt_error = rospy.Publisher("/cog_learning/cedar/mt_error", Image, queue_size=1, latch=True)
        self.pub_mt_lp = rospy.Publisher("/cog_learning/cedar/mt_lp", Image, queue_size=1, latch=True)
        self.pub_busy_out = rospy.Publisher("/cluster_msg/nnga/busy_out",Bool, queue_size=1, latch=True)
        self.pub_busy_act = rospy.Publisher("/cluster_msg/nnga/busy_action",Bool, queue_size=1, latch=True)
        self.pub_change_action = rospy.Publisher("/motion_pincher/change_action",Bool, queue_size=1, latch=True)
        self.pub_r_action = rospy.Publisher("/motion_pincher/good_action",Bool, queue_size=1, latch=True)
        self.pub_save_vae_out = rospy.Publisher("/habituation/save_vae_out",Bool, queue_size=1, latch=True)
        self.pub_save_vae_action = rospy.Publisher("/habituation/save_vae_action",Bool, queue_size=1, latch=True)
        self.load = rospy.get_param("load_nnga")
        self.folder_nnga = rospy.get_param("nnga_folder")
        self.nn_goalactions = []
        self.id_nnga = 0
        self.prev_id_nnga = -1 
        self.index_nnga = -1
        self.id_defined = False
        self.dmp = Dmp()
        self.sample_explore = SampleExplore()
        self.sample_exploit = SampleExploit()
        self.incoming_dmp = False
        self.incoming_outcome = False
        self.current_dmp = -1
        self.sample = False
        self.state = False
        self.tot_lout = 0
        self.tot_nout = 0
        self.tot_lact = 0
        self.tot_nact = 0
        self.tot_learning = 0
        self.tot_not_learning = 0
        self.tot_signal = 0
        self.busy_out_learning = False
        self.busy_out_not_learning = False
        self.busy_act_learning = False
        self.busy_act_not_learning = False
        self.busy_exploit = False
        self.busy_not_exploit = False
        self.send_signal = False
        self.lock = False
        self.changes = False
        rospy.Subscriber("/cluster_msg/sample_explore", SampleExplore, self.callback_sample_explore)
        rospy.Subscriber("/cluster_msg/sample_exploit", SampleExploit, self.callback_sample_exploit)
        rospy.Subscriber("/cog_learning/activate_goal", Goal, self.callback_latent)
        rospy.Subscriber("/cog_learning/id_object", Int16, self.callback_id)
        rospy.Subscriber("/cog_learning/mt", Image, self.field_callback)
        rospy.Subscriber("/cog_learning/mt_error", Image, self.error_callback)
        rospy.Subscriber("/cog_learning/mt_lp", Image, self.lp_callback)
        rospy.Subscriber("/cog_learning/outcome/learning", Float64, self.callback_learning_out)
        rospy.Subscriber("/cog_learning/outcome/not_learning", Float64, self.callback_not_out)
        rospy.Subscriber("/cog_learning/action/learning", Float64, self.callback_learning_act)
        rospy.Subscriber("/cog_learning/action/not_learning", Float64, self.callback_not_act)
        rospy.Subscriber("/cog_learning/exploitation/learning", Float64, self.callback_learning)
        rospy.Subscriber("/cog_learning/exploitation/not_learning", Float64, self.callback_not_learning)
        rospy.Subscriber("/cog_learning/action_reward", Float64, self.callback_reward)
        if self.load:
            self.load_memory()

    def callback_sample_explore(self, msg):
        print("NNGA got sample explore")
        self.sample_explore.state_x = msg.state_x
        self.sample_explore.state_y = msg.state_y
        self.sample_explore.state_angle = msg.state_angle
        self.sample_explore.v_x = msg.v_x
        self.sample_explore.v_y = msg.v_y
        self.sample_explore.v_pitch = msg.v_pitch
        self.sample_explore.roll = msg.roll
        self.sample_explore.grasp = msg.grasp
        self.sample_explore.fpos_x = msg.fpos_x
        self.sample_explore.fpos_y = msg.fpos_y
        self.sample_explore.fpos_pitch = msg.fpos_pitch
        self.sample_explore.outcome_x = msg.outcome_x
        self.sample_explore.outcome_y = msg.outcome_y
        self.sample_explore.outcome_angle = msg.outcome_angle
        self.sample_explore.outcome_touch = msg.outcome_touch

    def callback_sample_exploit(self, msg):
        print("NNGA got sample exploit")
        self.sample_exploit.fpos_x = msg.fpos_x
        self.sample_exploit.fpos_y = msg.fpos_y
        self.sample_exploit.fpos_pitch = msg.fpos_pitch
        self.sample_exploit.outcome_x = msg.outcome_x
        self.sample_exploit.outcome_y = msg.outcome_y
        self.sample_exploit.outcome_angle = msg.outcome_angle
        self.sample_exploit.outcome_touch = msg.outcome_touch
        self.sample_exploit.state_x = msg.state_x
        self.sample_exploit.state_y = msg.state_y
        self.sample_exploit.state_angle = msg.state_angle
        self.sample_exploit.dnf_x = msg.dnf_x
        self.sample_exploit.dnf_y = msg.dnf_y

    #get latent goal DNF, send dmp to controller, set current goal for fwd and inv model 
    def callback_latent(self, msg):
        print("got latent : ",msg)
        m = LatentGoalDnf()
        m.latent_x = msg.x
        m.latent_y = msg.y
        self.nn_goalactions[self.index_nnga].activate_hebbian(m)
        self.nn_goalactions[self.index_nnga].activate_dmp_actions(m)
        self.nn_goalactions[self.index_nnga].send_habituation(m)

    def callback_id(self, msg):
        if self.prev_id_nnga != self.id_nnga and msg.data != -1:
            self.id_nnga = msg.data
            found = False
            for i in range(0,len(self.nn_goalactions)):
                tmp = self.nn_goalactions[i].get_id()
                if tmp == self.id_nnga:
                    self.index_nnga = i
                    found = True
                    print("found object, sending latent space and MT...")
                    self.send_mt_field()
                    self.send_mt_error()
                    self.send_mt_lp()
            if not found:
                print("Creating new NNGA")
                goal_action = NNGoalAction(self.id_nnga)
                self.nn_goalactions.append(goal_action)
                self.index_nnga = len(self.nn_goalactions) - 1
                blank_mt = np.zeros((100,100,1), np.float32)
                self.nn_goalactions[self.index_nnga].set_mt_field(blank_mt)
                self.nn_goalactions[self.index_nnga].set_mt_error(blank_mt)
                self.nn_goalactions[self.index_nnga].set_mt_lp(blank_mt)
                self.send_mt_field()
                self.send_mt_error()
                self.send_mt_lp()
            self.prev_id_nnga = self.id_nnga
            self.id_defined = True
        if msg.data == -1:
            self.changes = True
        if msg.data != -1 and self.changes:
            self.send_mt_field()
            self.send_mt_error()
            self.send_mt_lp()
            self.changes = False 

    def field_callback(self,msg):
        try:
            # Convert your ROS Image message to OpenCV2
            cv2_img = self.bridge.imgmsg_to_cv2(msg, "32FC1")
            if self.id_defined:
                self.nn_goalactions[self.index_nnga].set_mt_field(cv2_img)
        except CvBridgeError as e:
            print(e)

    def error_callback(self,msg):
        try:
            # Convert your ROS Image message to OpenCV2
            cv2_img = self.bridge.imgmsg_to_cv2(msg, "32FC1")
            if self.id_defined:
                self.nn_goalactions[self.index_nnga].set_mt_error(cv2_img)
        except CvBridgeError as e:
            print(e)

    def lp_callback(self,msg):
        try:
            # Convert your ROS Image message to OpenCV2
            cv2_img = self.bridge.imgmsg_to_cv2(msg, "32FC1")
            if self.id_defined:
                self.nn_goalactions[self.index_nnga].set_mt_lp(cv2_img)
        except CvBridgeError as e:
            print(e)

    def callback_learning_out(self,msg):
        if msg.data > 0.9:
            self.tot_lout += 1
        else:
            self.tot_lout = 0
            self.busy_out_learning = False
        if self.tot_lout > 10 and not self.busy_out_learning:
            self.busy_out_learning = True
            b = Bool()
            b.data = True
            self.pub_busy_out.publish(b)
            self.bootstrap_learning()
            b.data = False
            self.pub_busy_out.publish(b)


    def callback_not_out(self,msg):
        if msg.data > 0.9:
            self.tot_nout += 1
        else:
            self.tot_nout = 0
            self.busy_out_not_learning = False
        if self.tot_nout > 10 and not self.busy_out_not_learning:  
            self.busy_out_not_learning = True
            b = Bool()
            b.data = True
            self.pub_busy_out.publish(b)
            self.bootstrap_learning()
            b.data = False
            self.pub_busy_out.publish(b)

    def callback_learning_act(self,msg):
        if msg.data > 0.9:
            self.tot_lact += 1
        else:
            self.tot_lact = 0
            self.busy_act_learning = False
        if self.tot_lact > 10 and not self.busy_act_learning:
            self.busy_act_learning = True
            b = Bool()
            b.data = True
            self.pub_busy_act.publish(b)
            self.bootstrap_learning()
            b.data = False
            self.pub_busy_act.publish(b)

    def callback_not_act(self,msg):
        if msg.data > 0.9:
            self.tot_nact += 1
        else:
            self.tot_nact = 0
            self.busy_act_not_learning = False
        if self.tot_nact > 10 and not self.busy_act_not_learning:
            self.busy_act_not_learning = True
            b = Bool()
            b.data = True
            self.pub_busy_act.publish(b)
            self.bootstrap_learning()
            b.data = False
            self.pub_busy_act.publish(b)

    def callback_learning(self,msg):
        #pass
        if msg.data > 0.9:
            self.tot_learning += 1
            self.tot_signal = 0
        else:
            self.tot_learning = 0
            self.busy_exploit = False
        if self.tot_learning > 10 and not self.busy_exploit:
            self.busy_exploit = True
            print("LEARNING...")
            self.nn_goalactions[self.index_nnga].continue_learning(self.sample_exploit)
            self.send_change_action(True)
            self.send_valid_action(True)
            self.send_signal = True

    def callback_not_learning(self,msg):
        #pass
        if msg.data > 0.9:
            self.tot_not_learning += 1
            self.tot_signal = 0
        else:
            self.tot_not_learning = 0
            self.busy_not_exploit = False
        if self.tot_not_learning > 15 and not self.busy_not_exploit and not self.send_signal:
            self.busy_not_exploit = True
            print("LEARNING NOTHING")
            self.nn_goalactions[self.index_nnga].send_ready(True)
            #self.send_change_action(True)
            self.send_signal = True
        if self.tot_learning == 0 and self.tot_not_learning == 0:
            self.tot_signal += 1
            if self.tot_signal > 100:
                self.send_signal = False
                self.tot_signal = 0

    def callback_reward(self,msg):
        pass

    def send_save_vae_out(self,msg):
        b = Bool()
        b.data = msg
        self.pub_save_vae_out.publish(b)

    def send_save_vae_action(self,msg):
        b = Bool()
        b.data = msg
        self.pub_save_vae_action.publish(b)

    def bootstrap_learning(self):
        if self.busy_out_learning and self.busy_act_learning and not self.lock:
            #both learning
            self.lock = True
            print("LEARNING BOTH")
            self.send_save_vae_out(True)
            self.send_save_vae_action(True)
            self.nn_goalactions[self.index_nnga].bootstrap_learning(True,True,self.sample_explore)
            self.lock = False
        if self.busy_out_learning and self.busy_act_not_learning and not self.lock:
            #new outcome but old action
            self.lock = True
            print("LEARNING ONLY OUTCOME")
            self.send_save_vae_out(True)
            self.send_save_vae_action(False)
            self.nn_goalactions[self.index_nnga].bootstrap_learning(True,False,self.sample_explore)
            self.lock = False
        if self.busy_out_not_learning and self.busy_act_learning and not self.lock:
            #existing outcome and new action
            self.lock = True
            print("LEARNING ONLY ACTION")
            self.send_save_vae_out(False)
            self.send_save_vae_action(True)
            self.nn_goalactions[self.index_nnga].bootstrap_learning(False,True,self.sample_explore)
            self.lock = False
        if self.busy_out_not_learning and self.busy_act_not_learning and not self.lock:
            #not learning AE but create new hebbian connections between outcome and action
            self.lock = True
            print("LEARNING NOTHING")
            self.send_save_vae_out(False)
            self.send_save_vae_action(False)
            self.nn_goalactions[self.index_nnga].bootstrap_learning(False,False,self.sample_explore)
            self.lock = False

    def send_mt_field(self):
        img_field = self.nn_goalactions[self.index_nnga].get_mt_field()
        img_msg = self.bridge.cv2_to_imgmsg(img_field, encoding="passthrough")
        self.pub_field.publish(img_msg)

    def send_mt_error(self):
        img_field = self.nn_goalactions[self.index_nnga].get_mt_error()
        img_msg = self.bridge.cv2_to_imgmsg(img_field, encoding="passthrough")
        self.pub_mt_error.publish(img_msg)

    def send_mt_lp(self):
        img_field = self.nn_goalactions[self.index_nnga].get_mt_lp()
        img_msg = self.bridge.cv2_to_imgmsg(img_field, encoding="passthrough")
        self.pub_mt_lp.publish(img_msg)

    def send_change_action(self, msg):
        d = Bool()
        d.data = msg
        self.pub_change_action.publish(d)

    def send_valid_action(self, msg):
        d = Bool()
        d.data = msg
        self.pub_r_action.publish(d)

    def get_prediction(self, req):
        res = PredActionResponse()
        for i in range(0,len(req.list_sample)):
            s = req.list_sample[i]
            out = self.nn_goalactions[self.index_nnga].predict_reward(s)
            print("prediction : ",out)
            res.outputs.append(out)
        
        return res
    
    def get_inverse(self, req):
        res = PredInverseResponse()
        inp = []
        for i in range(0,len(req.inputs)):
            inp.append(req.inputs[i])
        res.outputs = self.nn_goalactions[self.index_nnga].predict_inverse(inp)

        return res
    
    def get_inverse_error(self, req):
        res = GetInvErrorResponse()
        res.error_inv = self.nn_goalactions[self.index_nnga].get_inverse_error()

        return res    

    def load_memory(self):
        list_dir = os.listdir(self.folder_nnga)
        for i in range(0,len(list_dir)):
            tmp_nnga = NNGoalAction(i)
            tmp_nnga.load_memory(i)
            tmp_nnga.load_nn(i)
            tmp_nnga.load_skills(i)
            self.nn_goalactions.append(tmp_nnga)
        s = []
        l = []
        for i in self.nn_goalactions:
            s = i.get_skills()
            l = i.get_latent_space_dnf()
            print("nnga number : ",i.get_id())
            print("Latent DNF : ",l)
            #for i in s:
            #    print("skill : ",i.get_name())
            #    print("memory : ",i.get_memory())
        


        
if __name__ == "__main__":
    
    """nn_ga = NNGoalAction()
    goal = [0.1,0.1,0.5,0.2,0.3,0.3,0.1,0.1]
    sec_goal = [0.4,0.4,0.1,0.6,0.1]
    third_goal = [0.2,0.34,0.12,0.43,0.8]
    test_goal = [0.1,0.1,0.5,0.3,0.3]
    tensor_goal = torch.tensor(goal,dtype=torch.float)
    nn_ga.add_to_memory(tensor_goal)
    res = nn_ga.forward_encoder(tensor_goal)
    print("Inputs goal ",goal)
    print("Output encoder ",res)
    res2 = nn_ga.forward_decoder(res)
    print("Output decoder before training ",res2)
    nn_ga.trainDecoder()
    res3 = nn_ga.forward_decoder(res)
    print("Output decoder after training ",res3)
    res = nn_ga.forward_encoder(tensor_goal)
    print("Output encoder ",res)
    #print("RECONSTRUCTION 2",res2)
    #print("RECONSTRUCTION 3",res3)"""
    exploit = Exploitation()
    rospy.spin()