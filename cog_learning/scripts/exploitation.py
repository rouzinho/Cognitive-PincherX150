#!/usr/bin/env python3
import torch;
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
import numpy as np
import math
import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100
try:
    import cPickle as pickle
except ModuleNotFoundError:
    import pickle

is_cuda = torch.cuda.is_available()
#device = torch.device("cpu")

if not is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")

class MultiLayer(nn.Module):
    def __init__(self,input_layer,middle_layer1,middle_layer2,output_layer):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_layer, middle_layer1),
            nn.Tanh(),
            nn.Linear(middle_layer1, middle_layer2),
            nn.Tanh(),
            nn.Linear(middle_layer2, output_layer),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        return self.layers(x)
    
class NNGoalAction(object):
    def __init__(self):
        self.encoder = MultiLayer(8,6,4,2)
        self.encoder.to(device)
        self.decoder = MultiLayer(2,4,6,8)
        self.decoder.to(device)
        self.memory_size = 20
        self.memory = []

    def trainDecoder(self):
        current_cost = 0
        last_cost = 15
        learning_rate = 5e-3
        epochs = 150

        #self.inverse_model.to(device)
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.Adam(self.decoder.parameters(),lr=learning_rate)        
        current_cost = 0
        for i in range(0,1):
            self.decoder.train()
            optimizer.zero_grad()
            sample = self.memory[-1]
            sample = sample.to(device)
            inputs = self.encoder(sample)
            inputs = inputs.to(device)
            targets = sample
            targets = targets.to(device)
            outputs = self.decoder(inputs)
            cost = criterion(outputs,targets)
            cost.backward()
            optimizer.step()
            #current_cost = current_cost + cost.item()
        for i in range(0,epochs):
            for j in range(0,len(self.memory)):
                self.decoder.train()
                optimizer.zero_grad()
                sample = self.memory[j]
                sample = sample.to(device)
                inputs = self.encoder(sample)
                inputs = inputs.to(device)
                targets = sample
                targets = targets.to(device)
                outputs = self.decoder(inputs)
                cost = criterion(outputs,targets)
                cost.backward()
                optimizer.step()
                current_cost = cost.item()
            #print("Epoch: {}/{}...".format(i, epochs),"MSE : ",current_cost)

    def forward_encoder(self, data):
        data = data.to(device)
        output = self.encoder(data)

        return output
    
    def forward_decoder(self, data):
        data = data.to(device)
        output = self.decoder(data)

        return output

    def add_to_memory(self, data):
        self.memory.append(data)

class Skill(object):
    def __init__(self):
        self.inverse_model = MultiLayer(6,5,4,3)
        self.inverse_model.to(device)
        self.forward_model = MultiLayer(3,4,5,6)
        self.forward_model.to(device)
        self.memory_size = 20
        self.memory = []

    def addToMemory(self,sample):
        self.memory.append(sample)
        s = len(self.memory)
        if s > self.memory_size:
            self.memory.pop(0)

    def getMemory(self):
        return self.memory

    def trainInverseModel(self):
        current_cost = 0
        last_cost = 15
        learning_rate = 5e-3
        epochs = 1

        #self.inverse_model.to(device)
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.Adam(self.inverse_model.parameters(),lr=learning_rate)        
        current_cost = 0
        for i in range(0,1):
            self.inverse_model.train()
            optimizer.zero_grad()
            sample = self.memory[-1]
            inputs = sample[3]
            targets = sample[1]
            inputs = inputs.to(device)
            targets = targets.to(device)
            outputs = self.inverse_model(inputs)
            cost = criterion(outputs,targets)
            cost.backward()
            optimizer.step()
            #current_cost = current_cost + cost.item()
        for i in range(0,epochs):
            for j in range(0,len(self.memory)):
                self.inverse_model.train()
                optimizer.zero_grad()
                sample = self.memory[j]
                inputs = sample[3]
                targets = sample[1]
                inputs = inputs.to(device)
                targets = targets.to(device)
                outputs = self.inverse_model(inputs)
                cost = criterion(outputs,targets)
                cost.backward()
                optimizer.step()
                #current_cost = current_cost + cost.item()
            #print("Epoch: {}/{}...".format(i, epochs),
                                #"MSE : ",current_cost)

            #if current_cost > last_cost:
            #    break
            #last_cost = current_cost
            #current_cost = 0

    #takes object location and motor command as input and produces the expected future object location as output
    def trainForwardModel(self):
        current_cost = 0
        last_cost = 15
        learning_rate = 5e-3
        epochs = 1
        data_input = []
        self.forward_model.to(device)
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.Adam(self.forward_model.parameters(),lr=learning_rate)
        current_cost = 0
        for i in range(0,1):
            self.forward_model.train()
            optimizer.zero_grad()
            sample = self.memory[-1]
            inputs = sample[2]
            targets = sample[0]
            inputs = inputs.to(device)
            targets = targets.to(device)
            outputs = self.forward_model(inputs)
            cost = criterion(outputs,targets)
            cost.backward()
            optimizer.step()
            current_cost = current_cost + cost.item()
        for i in range(0,epochs):
            for j in range(0,len(self.memory)):
                self.forward_model.train()
                optimizer.zero_grad()
                sample = self.memory[j]
                inputs = sample[2]
                targets = sample[0]
                inputs = inputs.to(device)
                targets = targets.to(device)
                outputs = self.forward_model(inputs)
                cost = criterion(outputs,targets)
                cost.backward()
                optimizer.step()
                #current_cost = current_cost + cost.item()
            #print("Epoch: {}/{}...".format(i, epochs),
                                #"MSE : ",current_cost)

            #if current_cost > last_cost:
            #    break
            #last_cost = current_cost
            #current_cost = 0
            
        

    def saveNN(self):
        name_inv = "/home/altair/PhD/Codes/ExperimentIM-LCNE/datas/complete/inverse_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pt"
        torch.save(self.inverse_model, name_inv)
        name_fwd = "/home/altair/PhD/Codes/ExperimentIM-LCNE/datas/complete/forward_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pt"
        torch.save(self.forward_model, name_fwd)
    
    #compute the error between the prediction and the actual data
    def getErrorPrediction(self,prediction,actual):
        error = math.sqrt((actual[0][0]-prediction[0][0])**2 + (actual[0][1]-prediction[0][1])**2)

        return error

    def getErrorForward(self,prediction,actual):
        error = math.sqrt((actual[0][0]-prediction[0][0])**2 + (actual[0][1]-prediction[0][1])**2)

        return error

    #return the error between prediction and actual motor command for the inverse model
    #parameters are tensors
    def predictInverseModel(self,inputs,targets):
        self.inverse_model.eval()
        inputs = inputs.to(device)
        targets = targets.to(device)
        out = self.inverse_model(inputs)
        mse_loss = nn.MSELoss()
        error = mse_loss(out, targets)
        #error = self.getErrorPrediction(out,targets)

        return error

    def predictIVModel(self,inputs):
        self.inverse_model.eval()
        inputs = inputs.to(device)
        out = self.inverse_model(inputs)

        return out

    def predictForwardModel(self,inputs,targets):
        self.forward_model.eval()
        inputs = inputs.to(device)
        targets = targets.to(device)
        out = self.forward_model(inputs)
        mse_loss = nn.MSELoss()
        error = mse_loss(out, targets)
        #error = self.getErrorPrediction(out,targets)

        return error

    def saveMemory(self):
        #name = "/home/altair/PhD/catkin_noetic/rosbags/experiment/datas/goal_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pkl"
        name = "/home/altair/PhD/Codes/ExperimentIM-LCNE/datas/complete/goal_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pkl"
        exist = path.exists(name)
        if exist:
            os.remove(name)
        filehandler = open(name, 'wb')
        pickle.dump(self.memory, filehandler)
        #with open(name, 'wb') as outp:
        #    pickle.dump(self.memory, outp, pickle.HIGHEST_PROTOCOL)

    def retrieveMemory(self):
        #name = "/home/altair/PhD/catkin_noetic/rosbags/experiment/datas/goal_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pkl"
        #name = "/home/altair/PhD/Codes/ExperimentIM-LCNE/datas/neural_memory/goal_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pkl"
        name = "/home/altair/PhD/Codes/ExperimentIM-LCNE/datas/complete/goal_"+str(int(self.model_object.object))+"_"+str(int(self.model_object.goal))+".pkl"
        #with open(name, 'rb') as inp:
        #    mem = pickle.load(inp)
        filehandler = open(name, 'rb') 
        mem = pickle.load(filehandler)
        self.memory = mem

class VariationalEncoder(nn.Module):
    def __init__(self, latent_dims):
        super(VariationalEncoder, self).__init__()
        self.linear1 = nn.Linear(5, 3)
        self.linear2 = nn.Linear(3, latent_dims)
        self.linear3 = nn.Linear(3, latent_dims)

        self.N = torch.distributions.Normal(0, 1)
        #self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU
        #self.N.scale = self.N.scale.cuda()
        self.kl = 0

    def forward(self, x):
        #x = torch.flatten(x, start_dim=1)
        x = F.tanh(self.linear1(x))
        mu =  self.linear2(x)
        sigma = torch.exp(self.linear3(x))
        z = mu + sigma*self.N.sample(mu.shape)
        #self.kl = torch.mean(sigma**2 + mu**2 - torch.log(sigma) - 1/2)
        self.kl = -0.5 * torch.mean(1 + sigma - mu.pow(2) - sigma.exp())
        return z

class Decoder(nn.Module):
    def __init__(self, latent_dims):
        super(Decoder, self).__init__()
        self.linear1 = nn.Linear(latent_dims, 3)
        self.linear2 = nn.Linear(3, 5)

    def forward(self, z):
        z = F.tanh(self.linear1(z)) #F.relu
        z = torch.sigmoid(self.linear2(z))
        return z
        #return z.reshape((-1, 1, 28, 28))

class VariationalAutoencoder(nn.Module):
    def __init__(self, latent_dims):
        super(VariationalAutoencoder, self).__init__()
        self.encoder = VariationalEncoder(latent_dims)
        self.decoder = Decoder(latent_dims)

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)


def train(autoencoder, data, epochs=6000):
    kl_weight = 0.8
    opt = torch.optim.Adam(autoencoder.parameters())
    train_loss = 0.0
    stop = False
    i = 0
    nb_data = len(data)
    #for epoch in range(epochs):
    while not stop:
        for sample, y in data:
            s = sample.to(device) # GPU
            opt.zero_grad()
            x_hat = autoencoder(s)
            #loss = ((s - x_hat)**2).sum() + autoencoder.encoder.kl
            loss_mse = ((s - x_hat)**2).sum() 
            # KL divergence between encoder distrib. and N(0,1) distrib. 
            loss_kl = autoencoder.encoder.kl 
            # Get weighted loss
            loss = (loss_mse * (1 - kl_weight) + loss_kl * kl_weight)
            train_loss += loss.item()
            print("loss reconstruct : ",loss_mse)
            print("loss KL : ",loss_kl)
            loss.backward()
            opt.step()
            i += 1
            if nb_data > 1:
                if loss_mse < 9e-5 and loss_kl < 9e-5:
                    stop = True
                    print("epochs : ",i)
            else:
                if i > 10000:
                    stop = True
    return autoencoder

def plot_latent(autoencoder, data, num_batches=100):
    colors = []
    for i in range(0,10):
        for sample, col in data:
            z = autoencoder.encoder(sample)
            z = z.to('cpu').detach().numpy()
            #print(z)
            plt.scatter(z[0], z[1], c=col, cmap='tab10')

if __name__ == "__main__":
    torch.manual_seed(58)
    nn_ga = NNGoalAction()
    goal = [0.1,0.1,0.5,0.2,0.3,0.3,0.1,0.1]
    sec_goal = [0.4,0.4,0.1,0.6,0.1]
    third_goal = [0.2,0.34,0.12,0.43,0.8]
    test_goal = [0.1,0.1,0.5,0.3,0.3]
    tensor_goal = torch.tensor(goal,dtype=torch.float)
    nn_ga.add_to_memory(tensor_goal)
    res = nn_ga.forward_encoder(tensor_goal)
    print("Inputs goal ",goal)
    print("Output encoder ",res)
    res2 = nn_ga.forward_decoder(res)
    print("Output decoder before training ",res2)
    nn_ga.trainDecoder()
    res3 = nn_ga.forward_decoder(res)
    print("Output decoder after training ",res3)
    res = nn_ga.forward_encoder(tensor_goal)
    print("Output encoder ",res)
    #print("RECONSTRUCTION 2",res2)
    #print("RECONSTRUCTION 3",res3)