#!/usr/bin/env python3
import torch;
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
import numpy as np
import math
import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100
from cog_learning.nn_ga import *
from cog_learning.msg import LatentGoalDnf
from cog_learning.msg import LatentGoalNN
import rospy
from std_msgs.msg import Float64
from std_msgs.msg import Int16
from std_msgs.msg import Bool
from motion.msg import Dmp
from motion.msg import Action
from motion.msg import DmpAction
from motion.msg import DmpOutcome
from detector.msg import Outcome
from detector.msg import State


try:
    import cPickle as pickle
except ModuleNotFoundError:
    import pickle

is_cuda = torch.cuda.is_available()
#device = torch.device("cpu")

if not is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")

class Exploitation(object):
    def __init__(self):
        rospy.init_node('exploitation', anonymous=True)
        rospy.Subscriber("/cog_learning/exploration", Bool, self.callback_exploration)
        rospy.Subscriber("/cog_learning/exploitation", Bool, self.callback_exploitation)
        rospy.Subscriber("/cog_learning/dmp_outcome", DmpOutcome, self.callback_dmp_outcome)
        rospy.Subscriber("/cog_learning/object_state", State, self.callback_state)
        rospy.Subscriber("/cog_learning/latent_space", LatentGoalDnf, self.callback_latent)
        rospy.Subscriber("/motion_pincher/action_sample", Action, self.callback_sample)
        rospy.Subscriber("/cog_learning/id_object", Int16, self.callback_id)
        self.load = rospy.get_param("load")
        self.folder_nnga = rospy.get_param("nnga_folder")
        self.nn_goalactions = []
        self.id_nnga = -1
        self.index_nnga = -1
        self.explore = False
        self.exploitation = False
        self.dmp = Dmp()
        self.action_sample = Action()
        self.state_obj = State()
        self.dmp_outcome = DmpOutcome()
        self.outcome = Outcome()
        self.incoming_dmp = False
        self.incoming_outcome = False
        self.current_dmp = -1
        self.sample = False
        self.state = False
        if self.load:
            self.load_memory()

    def callback_exploration(self, msg):
        self.explore = msg.data

    def callback_exploitation(self, msg):
        self.exploitation = msg.data

    def callback_sample(self, msg):
        self.action_sample.lpos_x = msg.lpos_x
        self.action_sample.lpos_y = msg.lpos_y
        self.action_sample.lpos_pitch = msg.lpos_pitch
        self.sample = True

    def callback_state(self, msg):
        self.state_obj.state_x = msg.state_x
        self.state_obj.state_y = msg.state_y
        self.state_obj.state_angle = msg.state_angle
        self.state = True

    def callback_dmp_outcome(self, msg):
        self.dmp_outcome.v_x = msg.v_x
        self.dmp_outcome.v_y = msg.v_y
        self.dmp_outcome.v_pitch = msg.v_pitch
        self.dmp_outcome.roll = msg.roll
        self.dmp_outcome.grasp = msg.grasp
        self.dmp_outcome.x = msg.x
        self.dmp_outcome.y = msg.y
        self.dmp_outcome.angle = msg.angle
        self.dmp_outcome.touch = msg.touch
        if (self.sample and self.state):
            outcome = Outcome()
            outcome.state_x = self.state_obj.state_x
            outcome.state_y = self.state_obj.state_y
            outcome.state_angle = self.state_obj.state_angle
            outcome.x = msg.x
            outcome.y = msg.y
            outcome.angle = msg.angle
            outcome.touch = msg.touch
            dmp_action = DmpAction()
            dmp_action.v_x = msg.v_x
            dmp_action.v_y = msg.v_y
            dmp_action.v_pitch = msg.v_pitch
            dmp_action.roll = msg.roll
            dmp_action.grasp = msg.grasp
            dmp_action.lpos_x = self.action_sample.lpos_x
            dmp_action.lpos_y = self.action_sample.lpos_y
            dmp_action.lpos_pitch = self.action_sample.lpos_pitch
            self.nn_goalactions[self.index_nnga].bootstrap_learning(outcome,dmp_action)
            self.sample = False
            self.state = False
            
    #get latent goal, send dmp to controller, set current goal for fwd and inv model 
    def callback_latent(self, msg):
        print("Received Latent Goal : ",msg)
        goal_nn = LatentGoalNN()
        x = msg.latent_x / 100
        y = msg.latent_y / 100
        x_1 = self.nn_goalactions[self.index_nnga].scale_latent_to_reduce(x)
        y_1 = self.nn_goalactions[self.index_nnga].scale_latent_to_reduce(y)
        goal_nn.latent_x = self.nn_goalactions[self.index_nnga].scale_dnf_to_latent(x_1)
        goal_nn.latent_y = self.nn_goalactions[self.index_nnga].scale_dnf_to_latent(y_1)
        self.nn_goalactions[self.index_nnga].activate_dmp(goal_nn)
        self.nn_goalactions[self.index_nnga].activate_hebbian(msg)

    def callback_id(self, msg):
        self.id_nnga = msg.data
        found = False
        for i in range(0,len(self.nn_goalactions)):
            tmp = self.nn_goalactions[i].get_id()
            if tmp == self.id_nnga:
                self.index_nnga = i
                found = True
        if not found:
            print("Creating new NNGA")
            goal_action = NNGoalAction(self.id_nnga)
            self.nn_goalactions.append(goal_action)
            self.index_nnga = len(self.nn_goalactions) - 1

    def load_memory(self):
        list_dir = os.listdir(self.folder_nnga)
        for i in range(0,len(list_dir)):
            tmp_nnga = NNGoalAction(i)
            tmp_nnga.load_memory(i)
            tmp_nnga.load_nn(i)
            tmp_nnga.load_skills(i)
            self.nn_goalactions.append(tmp_nnga)
        s = []
        l = []
        for i in self.nn_goalactions:
            s = i.get_skills()
            l = i.get_latent_space_dnf()
        print("Latent DNF : ",l)
        for i in s:
            print("skill : ",i.get_name())
            print("memory : ",i.get_memory())
        


        
if __name__ == "__main__":
    
    """nn_ga = NNGoalAction()
    goal = [0.1,0.1,0.5,0.2,0.3,0.3,0.1,0.1]
    sec_goal = [0.4,0.4,0.1,0.6,0.1]
    third_goal = [0.2,0.34,0.12,0.43,0.8]
    test_goal = [0.1,0.1,0.5,0.3,0.3]
    tensor_goal = torch.tensor(goal,dtype=torch.float)
    nn_ga.add_to_memory(tensor_goal)
    res = nn_ga.forward_encoder(tensor_goal)
    print("Inputs goal ",goal)
    print("Output encoder ",res)
    res2 = nn_ga.forward_decoder(res)
    print("Output decoder before training ",res2)
    nn_ga.trainDecoder()
    res3 = nn_ga.forward_decoder(res)
    print("Output decoder after training ",res3)
    res = nn_ga.forward_encoder(tensor_goal)
    print("Output encoder ",res)
    #print("RECONSTRUCTION 2",res2)
    #print("RECONSTRUCTION 3",res3)"""
    exploit = Exploitation()
    rospy.spin()